{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Adult* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/2/adult). Extract the data files into the subdirectory: `../05_src/data/adult/` (relative to `./05_src/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Assuming that the files `adult.data` and `adult.test` are in `../05_src/data/adult/`, then you can use the code below to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 15)\n",
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income             int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "columns = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status',\n",
    "    'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "    'native-country', 'income'\n",
    "]\n",
    "adult_dt = (pd.read_csv('../../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "# I know Jesus said don't print stuff out in one of the sessions but I am doing this for my mental peace lol\n",
    "print(adult_dt.shape)       \n",
    "print(adult_dt.dtypes)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data:\n",
    "\n",
    "+ Create a dataframe `X` that holds the features (all columns that are not `income`).\n",
    "+ Create a dataframe `Y` that holds the target data (`income`).\n",
    "+ From `X` and `Y`, obtain the training and testing data sets:\n",
    "\n",
    "    - Use a train-test split of 70-30%. \n",
    "    - Set the random state of the splitting function to 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (22792, 14)\n",
      "X_test shape: (9769, 14)\n",
      "Y_train shape: (22792,)\n",
      "Y_test shape: (9769,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Creating my X (features) and Y (target)\n",
    "X = adult_dt.drop('income', axis=1)  # Drop the target column 'income' from the features\n",
    "Y = adult_dt['income']               # 'income' will be the target variable\n",
    "\n",
    "# Splitting data into training and testing sets (70-30% split)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Displaying the shape of the splits to confirm\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"Y_train shape:\", Y_train.shape)\n",
    "print(\"Y_test shape:\", Y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random States\n",
    "\n",
    "Please comment: \n",
    "\n",
    "+ What is the [random state](https://scikit-learn.org/stable/glossary.html#term-random_state) of the [splitting function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)? \n",
    "+ Why is it [useful](https://en.wikipedia.org/wiki/Reproducibility)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Comment here.)*\n",
    "> The random state in the train_test_split function is a parameter that controls the shuffling of data before splitting it into training and testing sets. By setting a specific value (e.g., random_state=42), it ensures that the split is reproducible, meaning youâ€™ll get the same split each time you run the code. <br><br>This is useful for reproducibility in experiments and model comparisons, as it guarantees consistent results across different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create a [Column Transformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that treats the features as follows:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * Apply [KNN-based imputation for completing missing values](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html):\n",
    "        \n",
    "        + Consider the 7 nearest neighbours.\n",
    "        + Weight each neighbour by the inverse of its distance, causing closer neigbours to have more influence than more distant ones.\n",
    "    * [Scale features using statistics that are robust to outliers](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler).\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * Apply a [simple imputation strategy](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer):\n",
    "\n",
    "        + Use the most frequent value to complete missing values, also called the *mode*.\n",
    "\n",
    "    * Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html):\n",
    "        \n",
    "        + Handle unknown labels if they exist.\n",
    "        + Drop one column for binary variables.\n",
    "    \n",
    "    \n",
    "The column transformer should look like this:\n",
    "\n",
    "![](./images/assignment_2__column_transformer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed X_train shape: (22792, 100)\n",
      "Transformed X_test shape: (9769, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Defining numerical and categorical columns here\n",
    "numerical_columns = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_columns = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', \n",
    "                       'race', 'sex', 'native-country']\n",
    "\n",
    "# Defining my transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=7, weights='uniform')),  # KNN Imputation\n",
    "    ('scaler', RobustScaler())  # Robust scaling\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Simple imputation (mode)\n",
    "    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))  # One-hot encoding\n",
    "])\n",
    "\n",
    "# Creating a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Applying columntransformer\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Verifying the shape of transformed data - for my own reference\n",
    "print(\"Transformed X_train shape:\", X_train_transformed.shape)\n",
    "print(\"Transformed X_test shape:\", X_test_transformed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `classifier` and assign a [`RandomForestClassifier()`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to it.\n",
    "\n",
    "The pipeline looks like this:\n",
    "\n",
    "![](./images/assignment_2__pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline created and model fitted successfully.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Defining model pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessor),               # Using ColumnTransformer\n",
    "    ('classifier', RandomForestClassifier())       # Adding RandomForestClassifier\n",
    "])\n",
    "\n",
    "# Fitting the pipeline to my training data\n",
    "model_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Predicting using my test data\n",
    "Y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# CHecking if it has worked - I need constant reassurances, sorry\n",
    "print(\"Pipeline created and model fitted successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Evaluate the model pipeline using [`cross_validate()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html):\n",
    "\n",
    "+ Measure the following [preformance metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values): negative log loss, ROC AUC, accuracy, and balanced accuracy.\n",
    "+ Report the training and validation results. \n",
    "+ Use five folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TazeenQ\\.conda\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "c:\\Users\\TazeenQ\\.conda\\envs\\dsi_participant\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:242: UserWarning: Found unknown categories in columns [7] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation results:\n",
      "neg_log_loss - Training: -0.0813 Â± 0.0003\n",
      "neg_log_loss - Validation: -0.3759 Â± 0.0136\n",
      "roc_auc - Training: 1.0000 Â± 0.0000\n",
      "roc_auc - Validation: 0.9040 Â± 0.0019\n",
      "accuracy - Training: 0.9999 Â± 0.0000\n",
      "accuracy - Validation: 0.8537 Â± 0.0049\n",
      "balanced_accuracy - Training: 0.9998 Â± 0.0001\n",
      "balanced_accuracy - Validation: 0.7753 Â± 0.0055\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Defining all my performance metrics\n",
    "scoring = {\n",
    "    'neg_log_loss': 'neg_log_loss',\n",
    "    'roc_auc': 'roc_auc',\n",
    "    'accuracy': 'accuracy',\n",
    "    'balanced_accuracy': 'balanced_accuracy'\n",
    "}\n",
    "\n",
    "# Cross validating\n",
    "cv_results = cross_validate(model_pipeline, X_train, Y_train, cv=5, scoring=scoring, return_train_score=True)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Cross-validation results:\")\n",
    "for metric in scoring.keys():\n",
    "    print(f\"{metric} - Training: {cv_results['train_' + metric].mean():.4f} Â± {cv_results['train_' + metric].std():.4f}\")\n",
    "    print(f\"{metric} - Validation: {cv_results['test_' + metric].mean():.4f} Â± {cv_results['test_' + metric].std():.4f}\")\n",
    "\n",
    "    #ignoring the two warnings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the fold-level results as a pandas data frame and sorted by negative log loss of the test (validation) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train_neg_log_loss  test_neg_log_loss  train_roc_auc  test_roc_auc  \\\n",
      "4             -0.0813            -0.3789            1.0        0.9037   \n",
      "2             -0.0812            -0.3772            1.0        0.9050   \n",
      "1             -0.0815            -0.3765            1.0        0.9045   \n",
      "3             -0.0814            -0.3748            1.0        0.9038   \n",
      "0             -0.0810            -0.3720            1.0        0.9030   \n",
      "\n",
      "   train_accuracy  test_accuracy  train_balanced_accuracy  \\\n",
      "4          0.9999         0.8534                   0.9998   \n",
      "2          0.9999         0.8535                   0.9998   \n",
      "1          0.9999         0.8525                   0.9998   \n",
      "3          0.9999         0.8542                   0.9998   \n",
      "0          0.9999         0.8540                   0.9998   \n",
      "\n",
      "   test_balanced_accuracy  \n",
      "4                  0.7753  \n",
      "2                  0.7752  \n",
      "1                  0.7745  \n",
      "3                  0.7755  \n",
      "0                  0.7760  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Providing my cross validation results from above as a dictionary\n",
    "cv_results_provided = {\n",
    "    'train_neg_log_loss': [-0.0810, -0.0815, -0.0812, -0.0814, -0.0813],\n",
    "    'test_neg_log_loss': [-0.3720, -0.3765, -0.3772, -0.3748, -0.3789],\n",
    "    'train_roc_auc': [1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    'test_roc_auc': [0.9030, 0.9045, 0.9050, 0.9038, 0.9037],\n",
    "    'train_accuracy': [0.9999, 0.9999, 0.9999, 0.9999, 0.9999],\n",
    "    'test_accuracy': [0.8540, 0.8525, 0.8535, 0.8542, 0.8534],\n",
    "    'train_balanced_accuracy': [0.9998, 0.9998, 0.9998, 0.9998, 0.9998],\n",
    "    'test_balanced_accuracy': [0.7760, 0.7745, 0.7752, 0.7755, 0.7753]\n",
    "}\n",
    "\n",
    "# Creating a DataFrame from the results\n",
    "cv_results_df = pd.DataFrame(cv_results_provided)\n",
    "\n",
    "# Sorting the DataFrame by the test set's negative log loss in ascending order\n",
    "cv_results_df_sorted = cv_results_df.sort_values(by='test_neg_log_loss', ascending=True)\n",
    "\n",
    "# Displaying the sorted DataFrame\n",
    "print(cv_results_df_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the mean of each metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_neg_log_loss        -0.08128\n",
       "test_neg_log_loss         -0.37588\n",
       "train_roc_auc              1.00000\n",
       "test_roc_auc               0.90400\n",
       "train_accuracy             0.99990\n",
       "test_accuracy              0.85352\n",
       "train_balanced_accuracy    0.99980\n",
       "test_balanced_accuracy     0.77530\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the mean of each metric\n",
    "cv_results_means = cv_results_df.mean()\n",
    "\n",
    "# Displaying results\n",
    "cv_results_means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the same performance metrics (negative log loss, ROC AUC, accuracy, and balanced accuracy) using the testing data `X_test` and `Y_test`. Display results as a dictionary.\n",
    "\n",
    "*Tip*: both, `roc_auc()` and `neg_log_loss()` will require prediction scores from `pipe.predict_proba()`. However, for `roc_auc()` you should only pass the last column `Y_pred_proba[:, 1]`. Use `Y_pred_proba` with `neg_log_loss()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg_log_loss': 0.377269131657552, 'roc_auc': 0.9002279305333587, 'accuracy': 0.8550516941345071, 'balanced_accuracy': 0.7752451035802832}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# Getting prediction probabilities test set\n",
    "Y_pred_proba = model_pipeline.predict_proba(X_test)\n",
    "\n",
    "# Calculating metrics\n",
    "test_metrics = {\n",
    "    'neg_log_loss': log_loss(Y_test, Y_pred_proba),\n",
    "    'roc_auc': roc_auc_score(Y_test, Y_pred_proba[:, 1]),\n",
    "    'accuracy': accuracy_score(Y_test, model_pipeline.predict(X_test)),\n",
    "    'balanced_accuracy': balanced_accuracy_score(Y_test, model_pipeline.predict(X_test))\n",
    "}\n",
    "\n",
    "# Displaying results\n",
    "print(test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Recoding\n",
    "\n",
    "In the first code chunk of this document, we loaded the data and immediately recoded the target variable `income`. Why is this [convenient](https://scikit-learn.org/stable/modules/model_evaluation.html#binary-case)?\n",
    "\n",
    "The specific line was:\n",
    "\n",
    "```\n",
    "adult_dt = (pd.read_csv('../05_src/data/adult/adult.data', header = None, names = columns)\n",
    "              .assign(income = lambda x: (x.income.str.strip() == '>50K')*1))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Answer here.)\n",
    ">Recoding income as 0 and 1 makes it easier to use with machine learning models and evaluation metrics, which expect numerical labels for binary classification. This avoids extra conversions later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_2_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "ðŸš¨ **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** ðŸš¨ for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [x ] Created a branch with the correct naming convention.\n",
    "- [x ] Ensured that the repository is public.\n",
    "- [x ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [x ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Becker,Barry and Kohavi,Ronny. (1996). Adult. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
